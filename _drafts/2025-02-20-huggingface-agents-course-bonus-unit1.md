---
title: "Hugging Face AI Agent Course - Bonus Unit1. Fine-tuning an LLM for function-calling"
date: "2025-02-20 23:01:17 +0900"
last_modified_at: "2025-02-20 23:01:17 +0900"
---

## Introduction
### What you'll learn

#### è¦ç´„
å½¹ç«‹ã¤ã¾ã¨ã‚ã‚’æä¾›ã—ã¾ã™ã®ã§ã€å†…å®¹ã®ãƒã‚¤ãƒ©ã‚¤ãƒˆã‚’ç°¡å˜ã«ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

ã€Œãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã€ã¨ã¯ã€ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦ LLM ã‚’ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºã™ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚

LLM ã®é–¢æ•°å‘¼ã³å‡ºã—ã‚’å¾®èª¿æ•´ã™ã‚‹ã“ã¨ã§ã€ä»¥ä¸‹ãŒå¯èƒ½ã«ãªã‚Šã¾ã™ã€‚

* ãƒ¢ãƒ‡ãƒ«ãŒã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«è¦³æ¸¬ã‚’è§£é‡ˆã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹ã€‚
* ã“ã‚Œã«ã‚ˆã‚Šã€AI ãŒã‚ˆã‚Šå …ç‰¢ã«ãªã‚Šã¾ã™ã€‚

ã“ã®ãƒœãƒ¼ãƒŠã‚¹ãƒ¦ãƒ‹ãƒƒãƒˆã¯ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã‚ã‚Šã€ãƒ¦ãƒ‹ãƒƒãƒˆ 1 ã‚ˆã‚Šã‚‚é«˜åº¦ã§ã™ã€‚

ã“ã®ãƒœãƒ¼ãƒŠã‚¹ãƒ¦ãƒ‹ãƒƒãƒˆã§ã¯æ¬¡ã®ã“ã¨ã‚’å­¦ã³ã¾ã™ã€‚

* é–¢æ•°å‘¼ã³å‡ºã—
* LoRA (Low-Rank Adaptation)
* é–¢æ•°å‘¼ã³å‡ºã—ãƒ¢ãƒ‡ãƒ«ã«ãŠã‘ã‚‹æ€è€ƒ â†’ è¡Œå‹• â†’ è¦³å¯Ÿã‚µã‚¤ã‚¯ãƒ«
* æ–°ã—ã„ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³

ã“ã®ãƒœãƒ¼ãƒŠã‚¹ãƒ¦ãƒ‹ãƒƒãƒˆã®å®Œäº†ã¾ã§ã«ã€æ¬¡ã®ã“ã¨ãŒã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚

* ãƒ„ãƒ¼ãƒ«ã¨ã® API ã®å†…éƒ¨å‹•ä½œã‚’ç†è§£ã™ã‚‹ã€‚
* LoRA ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã€‚
* å …ç‰¢ã§ä¿å®ˆå¯èƒ½ãªé–¢æ•°å‘¼ã³å‡ºã—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ä½œæˆã™ã‚‹ãŸã‚ã«æ€è€ƒ â†’ è¡Œå‹• â†’ è¦³å¯Ÿã‚µã‚¤ã‚¯ãƒ«ã‚’å®Ÿè£…ãŠã‚ˆã³å¤‰æ›´ã™ã‚‹ã€‚
* ãƒ¢ãƒ‡ãƒ«ã®å†…éƒ¨çš„ãªæ¨è«–ã¨å¤–éƒ¨çš„ãªè¡Œå‹•ã‚’ã‚·ãƒ¼ãƒ ãƒ¬ã‚¹ã«åŒºåˆ¥ã™ã‚‹ãŸã‚ã«ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¨­è¨ˆã—ã¦åˆ©ç”¨ã™ã‚‹ã€‚
* é–¢æ•°å‘¼ã³å‡ºã—ã‚’è¡Œã†ã‚ˆã†ã«ç‹¬è‡ªã®ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã€‚

## What is Function Calling?
### How does the model "learn" to take an action?
### è¦ç´„
é–¢æ•°å‘¼ã³å‡ºã—ã¯ã€LLMï¼ˆå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ï¼‰ãŒå¤–éƒ¨ç’°å¢ƒã¨å¯¾è©±ã™ã‚‹ãŸã‚ã®æ‰‹æ®µã§ã™ã€‚GPT-4 ã§åˆã‚ã¦å°å…¥ã•ã‚Œã€ä»–ã®ãƒ¢ãƒ‡ãƒ«ã«ã‚‚å–ã‚Šå…¥ã‚Œã‚‰ã‚Œã¾ã—ãŸã€‚ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ãƒ„ãƒ¼ãƒ«ã®ã‚ˆã†ã«ã€é–¢æ•°å‘¼ã³å‡ºã—ã¯ãƒ¢ãƒ‡ãƒ«ã«ç’°å¢ƒã¸ã®åƒãã‹ã‘ã‚’å¯èƒ½ã«ã—ã¾ã™ã€‚ãŸã ã—ã€é–¢æ•°å‘¼ã³å‡ºã—èƒ½åŠ›ã¯ãƒ¢ãƒ‡ãƒ«ã«ã‚ˆã£ã¦å­¦ç¿’ã•ã‚Œã€ä»–ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆæŠ€è¡“ã‚ˆã‚Šã‚‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¸ã®ä¾å­˜åº¦ãŒä½ã„ã§ã™ã€‚

ãƒ¦ãƒ‹ãƒƒãƒˆ1ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã§ã¯ã€ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ã†ã“ã¨ã‚’å­¦ç¿’ã•ã›ãšã«ãƒ„ãƒ¼ãƒ«ãƒªã‚¹ãƒˆã‚’æä¾›ã—ã€ãƒ¢ãƒ‡ãƒ«ãŒãã‚Œã‚‰ã‚’ä½¿ã£ã¦è¨ˆç”»ã‚’ç«‹ã¦ã‚‹èƒ½åŠ›ã‚’ä¸€èˆ¬åŒ–ã§ãã‚‹ã“ã¨ã«ä¾å­˜ã—ã¦ã„ã¾ã—ãŸã€‚ä¸€æ–¹ã€é–¢æ•°å‘¼ã³å‡ºã—ã§ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã¯ãƒ„ãƒ¼ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã‚ˆã†ã«ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ï¼ˆè¨“ç·´ï¼‰ã•ã‚Œã¾ã™ã€‚

ãƒ¢ãƒ‡ãƒ«ã¯ã©ã®ã‚ˆã†ã«è¡Œå‹•ã‚’ã€Œå­¦ç¿’ã€ã™ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿãƒ¦ãƒ‹ãƒƒãƒˆ1ã§ã¯ã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã®ä¸€èˆ¬çš„ãªãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’èª¿ã¹ã¾ã—ãŸã€‚ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆã«ãƒ„ãƒ¼ãƒ«ã‚’æä¾›ã—ã€ã‚¯ã‚¨ãƒªã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã¯æ¬¡ã®ã‚µã‚¤ã‚¯ãƒ«ã‚’ç¹°ã‚Šè¿”ã—ã¾ã™ã€‚

* **æ€è€ƒ:** ç›®çš„ã‚’é”æˆã™ã‚‹ãŸã‚ã«ã¯ã©ã®ã‚ˆã†ãªè¡Œå‹•ã‚’ã¨ã‚‹å¿…è¦ãŒã‚ã‚‹ã‹ï¼Ÿ
* **è¡Œå‹•:** æ­£ã—ã„ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§è¡Œå‹•ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã€ç”Ÿæˆã‚’åœæ­¢ã™ã‚‹ã€‚
* **è¦³å¯Ÿ:** å®Ÿè¡Œçµæœã‚’å–å¾—ã™ã‚‹ã€‚

é€šå¸¸ã®APIã‚’ä»‹ã—ãŸãƒ¢ãƒ‡ãƒ«ã¨ã®ä¼šè©±ã§ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒäº¤äº’ã«ã‚„ã‚Šå–ã‚Šã•ã‚Œã¾ã™ã€‚é–¢æ•°å‘¼ã³å‡ºã—ã¯ã“ã®ä¼šè©±ã«æ–°ã—ã„å½¹å‰²ã‚’ã‚‚ãŸã‚‰ã—ã¾ã™ã€‚è¡Œå‹•ã¨è¦³å¯Ÿãã‚Œãã‚Œã«æ–°ã—ã„å½¹å‰²ãŒè¿½åŠ ã•ã‚Œã¾ã™ã€‚

å¤šãã®APIã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã¯å–ã‚‹ã¹ãè¡Œå‹•ã‚’ã€Œã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã—ã¾ã™ã€‚ãƒãƒ£ãƒƒãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã¯ã€ã“ã‚Œã‚’é–¢æ•°å‘¼ã³å‡ºã—ã®ãŸã‚ã®ç‰¹åˆ¥ãªãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦è¡¨ç¾ã—ã¾ã™ã€‚

ã“ã®ã‚³ãƒ¼ã‚¹ã§ã¯ã€é–¢æ•°å‘¼ã³å‡ºã—ã«ã¤ã„ã¦å†ã³èª¬æ˜ã—ã¾ã™ãŒã€ã‚ˆã‚Šæ·±ãç†è§£ã—ãŸã„å ´åˆã¯ã€é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚é–¢æ•°å‘¼ã³å‡ºã—ã®ä»•çµ„ã¿ã‚’ç†è§£ã—ãŸä¸Šã§ã€é–¢æ•°å‘¼ã³å‡ºã—æ©Ÿèƒ½ã‚’æŒãŸãªã„ãƒ¢ãƒ‡ãƒ« `google/gemma-2-2b-it` ã«ã€æ–°ã—ã„ç‰¹åˆ¥ãªãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§é–¢æ•°å‘¼ã³å‡ºã—æ©Ÿèƒ½ã‚’è¿½åŠ ã—ã¾ã™ã€‚ãã®ãŸã‚ã«ã¯ã€ã¾ãšãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã¨LoRAã‚’ç†è§£ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

## Letâ€™s Fine-Tune your model for function-calling
### è¦ç´„
ã“ã®æ–‡ç« ã¯ã€é–¢æ•°å‘¼ã³å‡ºã—ã«å¯¾å¿œã—ãŸå¤§è¦æ¨¡è¨€èªãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ–¹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¦ã„ã¾ã™ã€‚

ä¸»ãªãƒã‚¤ãƒ³ãƒˆã¯ï¼“ã¤ã§ã™ã€‚

1. **ãƒ¢ãƒ‡ãƒ«ã®è¨“ç·´æ®µéš:**  ã¾ãšå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰å­¦ç¿’ã•ã‚ŒãŸãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹: `google/gemma-2-2b`ï¼‰ã‚’ç”¨æ„ã—ã¾ã™ã€‚æ¬¡ã«ã€æŒ‡ç¤ºã«å¾“ã†ã‚ˆã†ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ï¼ˆä¾‹: `google/gemma-2-2b-it`ï¼‰ã‚’åˆ©ç”¨ã™ã‚‹ã®ãŒåŠ¹ç‡çš„ã§ã™ã€‚æœ€å¾Œã«ã€é–¢æ•°å‘¼ã³å‡ºã—ã®ãŸã‚ã®è¿½åŠ ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’è¡Œã„ã¾ã™ã€‚

2. **LoRAã‚’ç”¨ã„ãŸåŠ¹ç‡çš„ãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°:**  LoRA (Low-Rank Adaptation)ã¨ã„ã†æ‰‹æ³•ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€è¨“ç·´ã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã—ã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ã®è‰¯ã„ã€é«˜é€Ÿãªãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿç¾ã—ã¾ã™ã€‚

3. **ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã¸ã®ãƒªãƒ³ã‚¯:**  é–¢æ•°å‘¼ã³å‡ºã—ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ–¹æ³•ã‚’å­¦ã¶ãŸã‚ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¸ã®ãƒªãƒ³ã‚¯ãŒæä¾›ã•ã‚Œã¦ã„ã¾ã™ã€‚


è¦ç´„ã™ã‚‹ã¨ã€æ—¢å­˜ã®æŒ‡ç¤ºè¿½å¾“æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€LoRAã‚’ç”¨ã„ã¦åŠ¹ç‡çš„ã«é–¢æ•°å‘¼ã³å‡ºã—æ©Ÿèƒ½ã‚’è¿½åŠ ã™ã‚‹ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°æ–¹æ³•ã¨ãã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’ç´¹ä»‹ã—ã¦ã„ã‚‹è¨˜äº‹ã§ã™ã€‚

### How do we train our model for function-calling ?
### æ—¥æœ¬èªè¨³
ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ 3 ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã«åˆ†ã‘ã‚‰ã‚Œã¾ã™ã€‚

ãƒ¢ãƒ‡ãƒ«ã¯å¤§é‡ã®ãƒ‡ãƒ¼ã‚¿ã§äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚Œã¦ã„ã¾ã™ã€‚ãã®ã‚¹ãƒ†ãƒƒãƒ—ã®å‡ºåŠ›ã¯äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ãŸã¨ãˆã°ã€google/gemma-2-2bã§ã™ã€‚ã“ã‚Œã¯åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã§ã‚ã‚Šã€é©åˆ‡ãªæŒ‡ç¤ºã«å¾“ã†èƒ½åŠ›ãŒãªãã€æ¬¡ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’äºˆæ¸¬ã™ã‚‹æ–¹æ³•ã—ã‹çŸ¥ã‚Šã¾ã›ã‚“ã€‚

ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ£ãƒƒãƒˆã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã§å½¹ç«‹ã¦ã‚‹ã«ã¯ã€æŒ‡ç¤ºã«å¾“ã†ã‚ˆã†ã«å¾®èª¿æ•´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ä½œæˆè€…ã€ã‚ªãƒ¼ãƒ—ãƒ³ã‚½ãƒ¼ã‚¹ ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã€ã‚ãªãŸã€ã¾ãŸã¯èª°ã§ã‚‚ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã§ãã¾ã™ã€‚ãŸã¨ãˆã°ã€`google/gemma-2-2b-it` ã¯ã€ Gemma ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®èƒŒå¾Œã«ã‚ã‚‹ Google ãƒãƒ¼ãƒ ã«ã‚ˆã£ã¦æŒ‡ç¤ºèª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚

ãã®å¾Œã€ãƒ¢ãƒ‡ãƒ«ã¯ä½œæˆè€…ã®å¥½ã¿ã«åˆã‚ã›ã¦èª¿æ•´ã§ãã¾ã™ã€‚ãŸã¨ãˆã°ã€é¡§å®¢ã«å¯¾ã—ã¦å¤±ç¤¼ãªæ…‹åº¦ã‚’å–ã£ã¦ã¯ãªã‚‰ãªã„ã‚«ã‚¹ã‚¿ãƒãƒ¼ ã‚µãƒ¼ãƒ“ã‚¹ ãƒãƒ£ãƒƒãƒˆ ãƒ¢ãƒ‡ãƒ«ãªã©ã§ã™ã€‚

é€šå¸¸ã€Gemini ã‚„ Mistral ã®ã‚ˆã†ãªå®Œæˆå“ã¯3 ã¤ã®ã‚¹ãƒ†ãƒƒãƒ—ã™ã¹ã¦ã‚’çµŒã¾ã™ãŒã€Hugging Face ã§è¦‹ã¤ã‹ã‚‹ãƒ¢ãƒ‡ãƒ«ã¯ã€ã“ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã® 1 ã¤ä»¥ä¸Šã®ã‚¹ãƒ†ãƒƒãƒ—ã«åˆæ ¼ã—ã¦ã„ã¾ã™ã€‚

ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€ `google/gemma-2-2b-it`ã«åŸºã¥ã„ã¦é–¢æ•°å‘¼ã³å‡ºã—ãƒ¢ãƒ‡ãƒ«ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã¯google/gemma-2-2bã§ã‚ã‚Šã€Google ãƒãƒ¼ãƒ ã¯æ¬¡ã®æ‰‹é †ã«å¾“ã£ã¦ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã€çµæœã¨ã—ã¦ `google/gemma-2-2b-it` ãŒç”Ÿã¾ã‚Œã¾ã—ãŸã€‚

ã“ã®å ´åˆã€ãƒ™ãƒ¼ã‚¹ãƒ¢ãƒ‡ãƒ«ã§ã¯ãªã `google/gemma-2-2b-it` ã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚ã“ã‚Œã¯ã€ãƒ¦ãƒ¼ã‚¹ã‚±ãƒ¼ã‚¹ã§ã¯äº‹å‰ã«è¡Œã‚ã‚ŒãŸå¾®èª¿æ•´ãŒé‡è¦ã§ã‚ã‚‹ãŸã‚ã§ã™ã€‚

ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã§ã®ä¼šè©±ã‚’é€šã˜ã¦ãƒ¢ãƒ‡ãƒ«ã¨å¯¾è©±ã—ãŸã„ã®ã§ã€åŸºæœ¬ãƒ¢ãƒ‡ãƒ«ã‹ã‚‰å§‹ã‚ã¦ã€æŒ‡ç¤ºã«å¾“ã†ã“ã¨ã€ãƒãƒ£ãƒƒãƒˆã€é–¢æ•°ã®å‘¼ã³å‡ºã—ã‚’å­¦ç¿’ã™ã‚‹ãŸã‚ã«ã€ã‚ˆã‚Šå¤šãã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¿…è¦ã«ãªã‚Šã¾ã™ã€‚

æŒ‡ç¤ºèª¿æ•´ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«(the instruct-tuned model)ã‹ã‚‰é–‹å§‹ã™ã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã™ã‚‹å¿…è¦ãŒã‚ã‚‹æƒ…å ±ã®é‡ã‚’æœ€å°é™ã«æŠ‘ãˆã¾ã™ã€‚

### LoRA  (Low-Rank Adaptation of Large Language Models)
### æ—¥æœ¬èªè¨³
LoRA (Low-Rank Adaptation of Large Language Models) ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ã‚’å¤§å¹…ã«å‰Šæ¸›ã™ã‚‹ã€äººæ°—ã®è»½é‡ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ‰‹æ³•ã§ã™ã€‚

ã“ã‚Œã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãƒ¢ãƒ‡ãƒ«ã«ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã¨ã—ã¦å°‘æ•°ã®æ–°ã—ã„é‡ã¿ã‚’æŒ¿å…¥ã™ã‚‹ã“ã¨ã§æ©Ÿèƒ½ã—ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€LoRA ã‚’ä½¿ç”¨ã—ãŸãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå¤§å¹…ã«é«˜é€ŸåŒ–ã•ã‚Œã€ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãŒå‘ä¸Šã—ã€ãƒ¢ãƒ‡ãƒ«ã®é‡ã¿ãŒå°ã•ããªã‚Š (æ•°ç™¾ MB)ã€ä¿å­˜ã¨å…±æœ‰ãŒå®¹æ˜“ã«ãªã‚Šã¾ã™ã€‚

LoRA ã¯ã€Transformer ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«ãƒ©ãƒ³ã‚¯åˆ†è§£è¡Œåˆ—ã®ãƒšã‚¢ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§æ©Ÿèƒ½ã—ã€é€šå¸¸ã¯ç·šå½¢ãƒ¬ã‚¤ãƒ¤ãƒ¼ã«é‡ç‚¹ãŒç½®ã‹ã‚Œã¾ã™ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®æ®‹ã‚Šã®éƒ¨åˆ†ã‚’ã€Œãƒ•ãƒªãƒ¼ã‚ºã€ã—ã€æ–°ã—ãè¿½åŠ ã•ã‚ŒãŸã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®é‡ã¿ã®ã¿ã‚’æ›´æ–°ã—ã¾ã™ã€‚

ãã†ã™ã‚‹ã“ã¨ã§ã€ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®é‡ã¿ã‚’æ›´æ–°ã™ã‚‹ã ã‘ã§æ¸ˆã‚€ãŸã‚ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«å¿…è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®æ•°ãŒå¤§å¹…ã«æ¸›å°‘ã—ã¾ã™ã€‚

æ¨è«–ä¸­ã€å…¥åŠ›ã¯ã‚¢ãƒ€ãƒ—ã‚¿ã¨ãƒ™ãƒ¼ã‚¹ ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã•ã‚Œã‚‹ã‹ã€ã“ã‚Œã‚‰ã®ã‚¢ãƒ€ãƒ—ã‚¿ã®é‡ã¿ã‚’ãƒ™ãƒ¼ã‚¹ ãƒ¢ãƒ‡ãƒ«ã¨ãƒãƒ¼ã‚¸ã§ãã‚‹ãŸã‚ã€è¿½åŠ ã®ãƒ¬ã‚¤ãƒ†ãƒ³ã‚· ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã¯ç™ºç”Ÿã—ã¾ã›ã‚“ã€‚

LoRA ã¯ã€ãƒªã‚½ãƒ¼ã‚¹è¦ä»¶ã‚’ç®¡ç†å¯èƒ½ãªç¯„å›²ã«ç¶­æŒã—ãªãŒã‚‰ã€å¤§è¦æ¨¡ãªè¨€èªãƒ¢ãƒ‡ãƒ«ã‚’ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã¾ãŸã¯ãƒ‰ãƒ¡ã‚¤ãƒ³ã«é©å¿œã•ã›ã‚‹ã®ã«ç‰¹ã«å½¹ç«‹ã¡ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«å¿…è¦ãªãƒ¡ãƒ¢ãƒªã‚’å‰Šæ¸›ã§ãã¾ã™ã€‚

---

è©³ã—ãã¯ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’ã¿ã¦ãã‚Œ -> https://huggingface.co/learn/nlp-course/chapter11/4?fw=pt

### Fine-Tuning a model for Function-calling
ã“ã‚Œã¯Colab ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’ä½¿ã£ã¦ã‚„ã£ã¦ã„ãã‚‰ã—ã„
https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit1/bonus-unit1.ipynb

To access Gemma on Hugging Face:

1. Make sure you're signed in to your Hugging Face Account
2. Go to https://huggingface.co/google/gemma-2-2b-it
3. Click on Acknowledge license and fill the form.

#### Step 1: Set the GPU
Open in Colab ã‚’æŠ¼ã—ã¦ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã‚’é–‹ã  
GPU ã¯ T4 GPU ã§  

ä»¥é™ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§è¡Œã†

#### Step2: Install dependencies
Notebook ã§ãƒãƒã£ã¦ Python libraries ã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹  

- bitsandbytes for quantization
- peftfor LoRA adapters
- Transformersfor loading the model
- datasetsfor loading and using the fine-tuning dataset
- trlfor the trainer class

#### Step 3: Create your Hugging Face Token to push your model to the Hub
HF ã®èªè¨¼ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½œæˆã™ã‚‹ã€‚https://huggingface.co/settings/tokens  
Write role ã§ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ä½œæˆã™ã‚‹

#### Step 4: Import the librairies
ã“ã‚Œã‚‚ãƒãƒã£ã¦ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã™ã‚‹
å…ˆã»ã©ä½œæˆã—ãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ç’°å¢ƒå¤‰æ•° `HF_TOKEN` ã«ã‚»ãƒƒãƒˆã™ã‚‹ã®ã ãŒã€ç›´æ¥æ›¸ã„ã¡ã‚ƒã£ã¦å¤§ä¸ˆå¤«ãªã®ã‹ï¼Ÿ

#### Step 5: Processing the dataset into inputs

> ã“ã®ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã§ã¯ã€`deepseek-ai/DeepSeek-R1-Distill-Qwen-32B`ã‹ã‚‰æ–°ã—ã„æ€è€ƒã‚¹ãƒ†ãƒƒãƒ—ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ã‚’è¿½åŠ ã™ã‚‹ã“ã¨ã§ã€é–¢æ•°å‘¼ã³å‡ºã—ã®ãŸã‚ã®ä¸€èˆ¬çš„ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ "NousResearch/hermes-function-calling-v1 "ã‚’æ‹¡å¼µã—ã¾ã—ãŸã€‚ ã—ã‹ã—ã€ãƒ¢ãƒ‡ãƒ«ãŒå­¦ç¿’ã™ã‚‹ãŸã‚ã«ã¯ã€ä¼šè©±ã‚’æ­£ã—ããƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚ Unit 1ã«å¾“ã£ãŸã®ã§ã‚ã‚Œã°ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã‹ã‚‰ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¸ã®ç§»å‹•ã¯`chat_template`ã«ã‚ˆã£ã¦å‡¦ç†ã•ã‚Œã‚‹ã“ã¨ã€ã‚ã‚‹ã„ã¯ã€gemma-2-2Bã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®`chat_template`ã«ã¯ãƒ„ãƒ¼ãƒ«ã®å‘¼ã³å‡ºã—ãŒå«ã¾ã‚Œã¦ã„ãªã„ã“ã¨ã‚’çŸ¥ã£ã¦ã„ã‚‹ã ã‚ã†ã€‚ ã¤ã¾ã‚Šã€gemma-2-2Bã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®`chat_template`ã«ã¯ãƒ„ãƒ¼ãƒ«ã®å‘¼ã³å‡ºã—ãŒå«ã¾ã‚Œã¦ã„ãªã„ã®ã§ã€ãã‚Œã‚’ä¿®æ­£ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼ã“ã‚ŒãŒãƒ—ãƒªãƒ—ãƒ­ã‚»ã‚¹é–¢æ•°ã®å½¹å‰²ã§ã™ã€‚ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®ãƒªã‚¹ãƒˆã‹ã‚‰ã€ãƒ¢ãƒ‡ãƒ«ãŒç†è§£ã§ãã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«å¤‰æ›ã™ã‚‹ã€‚

notebook ã‚’å®Ÿè¡Œã™ã‚‹ã®ã ãŒè­¦å‘ŠãŒå‡ºã¦ã„ã‚‹  
The secret `HF_TOKEN` does not exist in your Colab secrets.

ã†ãƒ¼ã‚“ã€è¨­å®šã¯ã—ãŸã‘ã©ãªï¼Ÿ

ä¸€å¿œãƒ–ãƒ©ã‚¦ã‚¶ã‚’ãƒªãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã€‚Step3 ã¾ã§ã¯çµ‚ã‚ã£ã¦ã„ã‚‹ã‚ˆã†ã ã‹ã‚‰ã€Step4 ã‚’å†åº¦å®Ÿè¡Œã€‚ãƒˆãƒ¼ã‚¯ãƒ³ã®ã‚»ãƒƒãƒˆã‚‚å¿˜ã‚Œãšã«
ãã—ã¦ Step5 ã§ãƒãƒã‚‹ãŒã‚‚ã†å®Ÿè¡Œæ¸ˆã¿ãªã®ã§ä½•ã‚‚èµ·ã“ã‚‰ãªã„ãª

#### Step 6: A Dedicated Dataset for This Unit

ã“ã®ãƒœãƒ¼ãƒŠã‚¹ãƒ¦ãƒ‹ãƒƒãƒˆã§ã¯ã€ãƒ•ã‚¡ãƒ³ã‚¯ã‚·ãƒ§ãƒ³ãƒ»ã‚³ãƒ¼ãƒªãƒ³ã‚°ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹ã¨ã•ã‚Œã¦ã„ã‚‹NousResearch/hermes-function-calling-v1ã‚’ãƒ™ãƒ¼ã‚¹ã«ã€ã‚«ã‚¹ã‚¿ãƒ ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã—ãŸã€‚ ã‚ªãƒªã‚¸ãƒŠãƒ«ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ç´ æ™´ã‚‰ã—ã„ãŒã€ã€Œè€ƒãˆã‚‹ã€ã‚¹ãƒ†ãƒƒãƒ—ã¯å«ã¾ã‚Œã¦ã„ãªã„ã€‚

> é–¢æ•°å‘¼ã³å‡ºã—ã§ã¯ã€ã“ã®ã‚ˆã†ãªã‚¹ãƒ†ãƒƒãƒ—ã¯ä»»æ„ã§ã‚ã‚‹ãŒã€deepseekãƒ¢ãƒ‡ãƒ«ã‚„è«–æ–‡ "Test-Time Compute "ã®ã‚ˆã†ãªæœ€è¿‘ã®ç ”ç©¶ã§ã¯ã€LLMãŒç­”ãˆã‚’å‡ºã™å‰ï¼ˆã“ã®å ´åˆã¯ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’èµ·ã“ã™å‰ï¼‰ã« "è€ƒãˆã‚‹ "æ™‚é–“ã‚’ä¸ãˆã‚‹ã“ã¨ã§ã€ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ãŒå¤§å¹…ã«å‘ä¸Šã™ã‚‹ã“ã¨ãŒç¤ºå”†ã•ã‚Œã¦ã„ã‚‹ã€‚ ãã“ã§ç§ã¯ã€ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’è¨ˆç®—ã—ã€é–¢æ•°å‘¼ã³å‡ºã—ã®å‰ã«æ€è€ƒãƒˆãƒ¼ã‚¯ãƒ³<think>ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ã«ã€deepseek-ai/DeepSeek-R1-Distill-Qwen-32Bã«æ¸¡ã™ã“ã¨ã«ã—ãŸã€‚ ãã®çµæœã€æ¬¡ã®ã‚ˆã†ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå¾—ã‚‰ã‚ŒãŸï¼š

ï¼ˆã“ã“ã§ç”»åƒãŒè²¼ã‚‰ã‚Œã¦ã„ã‚‹ã‘ã©ã©ã†ã™ã‚‹ã‚“ã ï¼Ÿï¼‰ -> ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒã‚ã‚‹  

ãã—ã¦ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ãŸ  

```python
dataset = dataset.map(preprocess, remove_columns="messages")
dataset = dataset["train"].train_test_split(0.1)
print(dataset)
```

```
DatasetDict({
    train: Dataset({
        features: ['text'],
        num_rows: 3213
    })
    test: Dataset({
        features: ['text'],
        num_rows: 357
    })
})
```

#### Step 7: Checking the inputs
```
å…¥åŠ›ãŒã©ã®ã‚ˆã†ãªã‚‚ã®ã‹ã€æ‰‹å‹•ã§è¦‹ã¦ã¿ã‚ˆã†ï¼ã“ã®ä¾‹ã§ã¯ã€æ¬¡ã®ã‚ˆã†ãªã‚‚ã®ãŒã‚ã‚‹ï¼š <tools></tools>ã®é–“ã«åˆ©ç”¨å¯èƒ½ãªãƒ„ãƒ¼ãƒ«ã®ãƒªã‚¹ãƒˆã‚’å«ã‚€å¿…è¦ãªæƒ…å ±ã‚’å«ã‚€ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼š ã€Œ<think></think>ã«å«ã¾ã‚Œã‚‹ "thinking "ãƒ•ã‚§ãƒ¼ã‚ºã¨<tool_call></tool_call>ã«å«ã¾ã‚Œã‚‹ "Act "ãƒ•ã‚§ãƒ¼ã‚ºã§ã™ã€‚ ã‚‚ã—ãƒ¢ãƒ‡ãƒ«ãŒ<tools_call>ã‚’å«ã‚“ã§ã„ã‚Œã°ã€ãƒ„ãƒ¼ãƒ«ã‹ã‚‰ã®ç­”ãˆã¨<tool_response></tool_response>ã‚’å«ã‚€æ–°ã—ã„ "Tool "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«ã“ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®çµæœã‚’è¿½åŠ ã—ã¾ã™ã€‚
```

ãªã‚“ã‹ `<think>` ã®ã¨ã“ã‚ã§è€ƒãˆã¦ã„ã‚‹ã‚“ã ãªã¨ã„ã†ã“ã¨ãŒã‚ã‹ã‚‹

`<tool_call>` ã§ãƒ„ãƒ¼ãƒ«ã‚’èª­ã‚“ã§ã„ã‚‹ã€‚ãã®ã‚¿ã‚°ã®é–“ã«ã¯å¼•æ•°ãŒè¨˜è¿°ã•ã‚Œã¦ã„ã‚‹

```
<tool_call>
{'name': 'get_news_headlines', 'arguments': {'country': 'United States'}}
</tool_call>
```

```
<tool_response>
{'headlines': ['French President announces new environmental policy', 'Paris Fashion Week highlights', 'France wins World Cup qualifier', 'New culinary trend sweeps across France', 'French tech startup raises millions in funding']}
</tool_response>
```

`tool_response` ãŒã‚ã‚‹ã®ã§ãã‚Œã‚’ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã«è¿½åŠ ã—ã¦ã„ã‚‹

ä»¥ä¸‹ã‚‚è¡Œã†ã€‚ã‚µãƒ‹ãƒ†ã‚£ãƒã‚§ãƒƒã‚¯ã¨ãª  

```python
# Sanity check
print(tokenizer.pad_token)
print(tokenizer.eos_token)
```

#### ã‚¹ãƒ†ãƒƒãƒ—8ï¼šãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ä¿®æ­£ã™ã‚‹
#### Step8: Let's Modify the Tokenizer

> ç¢ºã‹ã«ã€ãƒ¦ãƒ‹ãƒƒãƒˆ1ã§è¦‹ãŸã‚ˆã†ã«ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚µãƒ–ãƒ¯ãƒ¼ãƒ‰ã«åˆ†å‰²ã—ã¾ã™ã€‚ ã“ã‚Œã¯ã€ç§ãŸã¡ãŒæ–°ã—ã„ç‰¹åˆ¥ãªãƒˆãƒ¼ã‚¯ãƒ³ã«æœ›ã‚€ã“ã¨ã§ã¯ã‚ã‚Šã¾ã›ã‚“ï¼<think>ã€<tool_call>ã€<tool_response>ã‚’ä½¿ç”¨ã—ã¦ç§ãŸã¡ã®ä¾‹ã‚’åˆ†å‰²ã—ã¾ã—ãŸãŒã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã¯ã¾ã ãã‚Œã‚‰ã‚’å…¨ä½“ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦æ‰±ã„ã¾ã›ã‚“ã€‚ ã•ã‚‰ã«ã€ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå†…ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¨ã—ã¦ä¼šè©±ã‚’ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã™ã‚‹ãŸã‚ã«ã€ãƒ—ãƒªãƒ—ãƒ­ã‚»ã‚¹é–¢æ•°ã®`chat_template`ã‚’å¤‰æ›´ã—ãŸã®ã§ã€ã“ã‚Œã‚‰ã®å¤‰æ›´ã‚’åæ˜ ã™ã‚‹ãŸã‚ã«ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®`chat_template`ã‚‚å¤‰æ›´ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚

#### Step 9: Let's configure the LoRA
> This is we are going to define the parameter of our adapter. Those a the most important parameters in LoRA as they define the size and importance of the adapters we are training.
> ã“ã“ã§ã€ã‚¢ãƒ€ãƒ—ã‚¿ãƒ¼ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’å®šç¾©ã—ã¾ã™ã€‚ LoRAã§æœ€ã‚‚é‡è¦ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã‚¢ãƒ€ãƒ—ã‚¿ã®ã‚µã‚¤ã‚ºã¨é‡è¦æ€§ã‚’å®šç¾©ã™ã‚‹ã€‚
#### ã‚¹ãƒ†ãƒƒãƒ—10ï¼š Trainerã¨Fine-Tuningãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã‚ˆã†
#### Step 10: Let's define the Trainer and the Fine-Tuning hyperparameters

> ã“ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ãŸã‚ã«ä½¿ã†ã‚¯ãƒ©ã‚¹ã§ã‚ã‚‹Trainerã¨ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã¾ã™ã€‚

ãªã‚“ã‹è¨­å®šã¨ã‹äº‹å‰æº–å‚™ã‚’ã—ã¦ã‚‹ã¿ãŸã„ã 

> As Trainer, we use the SFTTrainer which is a Supervised Fine-Tuning Trainer.

ãªã‚“ã‹ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’è‰²ã€…ã„ã˜ã£ã¦ã„ã‚‹ã‚ˆã†ã 

> Here, we launch the training ğŸ”¥. Perfect time for you to pause and grab a coffee â˜•.

ã„ã‚ˆã„ã‚ˆãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã£ã¦æ„Ÿã˜ã‹

ã“ã‚ŒãŒæ™‚é–“ã‹ã‹ã‚‹ã‹ã‚‰ä¸Šé™ã«é”ã—ã¦ã—ã¾ã£ãŸ

-> ã—ã‚‡ã†ãŒãªã„ã‹ã‚‰ Pay as you go ã§100ãƒ¦ãƒ‹ãƒƒãƒˆè³¼å…¥ã—ãŸ

ã‚‚ã†ä¸€åº¦åˆã‚ã‹ã‚‰ã‚„ã‚‹ã‹

#### Step 11: Let's push the Model and the Tokenizer to the Hub
> ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¦ã¿ã¾ã—ã‚‡ã†ï¼ ãƒ¢ãƒ‡ãƒ«ã¯ã€ãƒ¦ãƒ¼ã‚¶ãƒ¼åã¨å…ˆã»ã©æŒ‡å®šã—ãŸ `output_dir` ã®ä¸‹ã«ãƒ—ãƒƒã‚·ãƒ¥ã•ã‚Œã¾ã™ã€‚

`output_dir` ã¯å…ˆã»ã©ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸæ™‚ã«æŒ‡å®šã—ã¦ã„ã‚‹ãƒ‘ã‚¹

> ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã«å«ã¾ã‚Œã¦ã„ã‚‹`chat_template`ã‚‚ä¿®æ­£ã—ãŸã®ã§ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚‚ãƒ¢ãƒ‡ãƒ«ã¨ã¨ã‚‚ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã‚ˆã†ã€‚

```
403 Forbidden: You don't have the rights to create a model under the namespace "Jofthomas".
Cannot access content at: https://huggingface.co/api/repos/create.
Make sure your token has the correct permissions.
```
Step 10 ã§ username ã‚’ Jofthomas ã˜ã‚ƒãªãã¦è‡ªåˆ†ã®åå‰ã«ã—ãªã„ã¨ã„ã‘ãªã„ã®ã‹ã€‚ã§ã‚‚ `trainer.push_to_hub(f"{username}/{output_dir}")` ã§ `Jofthomas/<output_dir>` ã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¦ã—ã¾ã£ãŸ

ã‚‚ã†ä¸€åº¦åˆã‚ã‹ã‚‰ã ã€œ

## Conclusion
ãªã‚‹ã»ã©ã€ã“ã‚Œã ã‘ï¼Ÿãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¤ã„ã¦ã®ç°¡å˜ãªèª¬æ˜ã®ã¿ã§ã‚ã¨ã¯ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’æ¥½ã—ã‚“ã§ã­ï¼ã£ã¦ã“ã¨ã‹ãª
