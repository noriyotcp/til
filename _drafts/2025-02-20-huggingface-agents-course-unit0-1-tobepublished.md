---
title: "【体験記】Hugging Face AI エージェントコース Unit 1"
date: "2025-02-20 23:22:20 +0900"
last_modified_at: "2025-02-20 23:22:20 +0900"
---

# 【体験記】Hugging Face AI エージェントコース Unit 1  
～未来のタスク自動化への扉を開く体験レポート～

## はじめに

昨今、「AIエージェント」という言葉をよく耳にしますが、実際はどのような技術で動いているのでしょうか？  
私自身、Hugging Face が提供する AI エージェントコース Unit 1 を体験し、その仕組みや実践的なアプローチに大変驚かされました。  
この記事では、コードの具体例などは割愛しますが、講座で学べる全体像や魅力、そして受講後に広がる可能性について、分かりやすくお伝えします。

---

## 1. AI エージェントとは何か？

### 基本コンセプト

- **定義:**  
  AI エージェントは、大規模言語モデル（LLM）と外部ツール（例：天気情報取得や画像生成など）を組み合わせ、ユーザーの要望に従って自律的にタスクを実行するシステムです。  
- **従来のチャットボットとの違い:**  
  単に決まった応答を返すのではなく、状況に応じて思考し、行動し、フィードバックを受けることで、柔軟かつダイナミックにタスクを解決します。

---

## 2. 講座の魅力～技術の背景と学びの実践～

### LLM の仕組みを噛み砕いて解説

- **トークナイゼーションと自己回帰型予測:**  
  テキストを小さな単位（トークン）に変換し、次に来るべきトークンを予測するという基本的な流れは、複雑そうに見えますが、講座では丁寧に解説されます。
  
- **Transformer と Attention メカニズム:**  
  内部でどの部分に注目しているかを判断する仕組みは、エージェントが効率的に情報を整理し、最適な回答や動作を導き出す原動力となっています。

### ツールとアクションの連携

- **エージェントの「道具箱」:**  
  外部APIや様々なツールを使うことで、単なる言語処理にとどまらない、実世界のタスク（天気情報の取得、画像生成など）が可能になります。
  
- **自律的な判断と実行:**  
  エージェントは「思考（どの行動を取るか判断する部分）」と「行動（ツールを呼び出して実際にタスクを実行する部分）」が連携し、動的なフィードバックループで課題を解決します。

---

## 3. 講座体験がもたらすもの

### ハンズオン体験の魅力

- **実践重視:**  
  講座は単なる理論解説に留まらず、実際に手を動かしながら、エージェントの動作やツールの役割を体験できます。  
  「体験すること」で未来のタスク自動化やより高度なAIシステム開発のイメージが具体的に掴める点が大きな魅力です。

- **わかりやすい解説:**  
  講座は、技術の背景や仕組みを噛み砕いて伝えることに重点を置いているため、初学者でも理解しやすい内容になっています。

### コミュニティとの連携と認定の可能性

- **コミュニティ交流:**  
  Discord サーバーや GitHub プラットフォームを通じて、他の受講者や講師とアイデアを交換できる環境が整っています。
  
- **認定取得:**  
  講座を修了すると、実力を証明する認定が得られ、今後のキャリアアップやプロジェクトでの応用に繋がります。

---

## 4. 最後に～未来を切り拓く一歩をあなたに～

Hugging Face の AI エージェントコース Unit 1 は、現代の AI 技術と未来のタスク自動化の可能性を体感できる絶好の機会です。  
実際に手を動かしてエージェントがどのように動作するのかを学ぶことで、従来のチャットボットでは叶わなかった柔軟なシステム設計の世界に触れることができます。

**あなたも、この講座で未来のタスク自動化の最前線に飛び込んでみませんか？**

受講後の新たな発見や可能性が、あなたの技術とキャリアを大きく前進させることでしょう。

---

ぜひ、Hugging Face の AI エージェントコースを体験し、最先端の AI 技術をあなた自身のものにしてください！

---

# huggingface の Agents Course
2/10 から開講している  

certification process の締切は 5/1 まで


各章は週に３、4時間が目安  

https://huggingface.co/learn/agents-course/unit0/introduction


Hugging Faceが提供するこの無料AIエージェントコースは、AIエージェントの理論、設計、実践を網羅した包括的な学習プログラムです。  コースは、基礎的な概念から高度な応用まで段階的に進んでおり、初心者でも理解しやすいように設計されています。

**コースの主な内容:**

* **基礎ユニット:** AIエージェントの基本概念（ツール、思考、行動、観測とそのフォーマット、LLM、メッセージ、特殊トークン、チャットテンプレートなど）を解説します。Python関数を使ったシンプルなユースケースも含まれます。
* **フレームワーク:**  smolagents、LangGraph、LlamaIndexといった主要なAIエージェントライブラリの使用方法を学びます。
* **実践演習:**  Hugging Face Spaces上で事前に設定された環境を利用して、実際のAIエージェントを構築・訓練します。
* **ユースケース課題:**  学習した概念を応用し、現実世界の課題を解決するAIエージェントを構築します。
* **最終課題:**  選定されたベンチマークに対してエージェントを構築し、他の受講者と競争します。リーダーボードでの成績によって評価されます。
* **認定取得:**  コースの修了状況に応じて、基礎認定または修了認定を取得できます。基礎認定はユニット1の完了で、修了認定はユニット1、ユースケース課題1つ、最終課題の完了で取得可能です。認定取得期限は2025年5月1日です。


**学習環境とサポート:**

* **Discordサーバー:**  他の受講者や講師と交流し、質問や議論を行うことができます。
* **GitHub:**  コース内容のバグ報告や改善提案を行うことができます。
* **Hugging Faceアカウント:**  モデルやエージェントを共有し、Spacesを作成するために必要です。
* **推奨ペース:**  毎週3～4時間、約1週間で1つの章を完了することを推奨しています。


**前提条件:**

* Pythonの基本知識
* LLMの基本知識（コース内で復習も含まれます）


**その他:**

* コースは受講者のフィードバックに基づいて継続的に更新されます。
* ボーナスユニットが追加される予定です。


要するに、このコースは、AIエージェント開発の基礎から実践までを網羅した、実践的でコミュニティ重視の学習プログラムです。  認定取得を目指したり、単に知識を深めたい人にも最適なコースと言えるでしょう。

## Onboarding: Your First Steps 
このオンボーディングでは、Hugging Faceアカウントの作成、Discordサーバーへの参加と自己紹介、Hugging Face Agentsコースのフォロー、そしてコースの拡散の4ステップを案内しています。Discordサーバーでは、コースに関する様々なチャンネルが用意されており、コミュニティとの交流も促進されます。

## Unit 1. Introduction to Agents

### Introduction to Agents
このユニットでは、AIエージェントの基礎を学びます。エージェントとは何か、どのように意思決定を行うか、LLM（大規模言語モデル）がエージェントの「脳」としてどのように機能するか、ツールとアクションの使用方法、エージェントのワークフロー（Think→Act→Observe）などを学習します。  簡単なタスクを実行するエージェント「Alfred」の作成とHugging Face Spacesへの公開方法も習得できます。最後にクイズに合格すると、修了証が発行されます。2月12日午後5時（CET）にはライブQ&Aも予定されています。

### What is an Agent?
#### 要約
この文章は、AIエージェントの概要を説明しています。エージェントは、AIモデル（主に大規模言語モデル：LLM）を用いて環境と相互作用し、ユーザー定義の目標を達成するシステムです。

主なポイントは以下です。

* **エージェントの構成**:  「脳」にあたるAIモデル（思考、計画）と、「体」にあたる能力・ツール（行動実行）から成る。
* **AIモデル**: LLMが一般的だが、画像認識もできるVLMなども利用可能。
* **環境との相互作用**:  ツール（メール送信、画像生成など）を使って行動し、結果を観察する。
* **タスク例**: バーチャルアシスタント、カスタマーサービスチャットボット、ゲームのNPCなど。
* **エージェントの機能**: 自然言語理解、推論・計画、環境との相互作用。


要するに、エージェントは指示を理解し、計画を立て、行動して目標を達成するAIシステムです。

Tools と Actions との違いや関係：  

文章によると、Tools と Actions は密接に関連していますが、同一ではありません。

* **Tools:** エージェントが環境とやりとりするために利用できる具体的な機能や外部リソースのことです。  メールを送信する関数、ウェブ検索を行う機能、画像を生成するAPIなど、具体的な作業を実行するものです。  エージェントはこれらのツールを組み合わせてタスクを実行します。  いわば、エージェントの「道具箱」の中身です。

* **Actions:** エージェントが行う行動のことです。「コーヒーを作る」「メールを送信する」「情報を検索する」など、目標達成のために実行される具体的なステップです。  一つのActionは、複数のToolsを必要とする場合もあります。


簡単に言えば、**Actionsは「何をするか」であり、Toolsは「どうやってするか」**です。

例えば、「メールを送信する」(Action) というタスクを実行するために、エージェントは「メール送信ツール」(Tool) を使用します。このツールは、メールサーバーに接続し、メール本文を送信する具体的な機能を提供します。  別の例として、「画像を生成する」(Action) というタスクでは、「画像生成API」(Tool) を利用します。

つまり、ToolsはActionsを実行するための手段であり、エージェントの能力を拡張するものです。  適切なToolsがなければ、エージェントは特定のActionsを実行できません。  Toolsの設計はエージェントの性能に大きく影響を与えます。

### What are LLMs?
### What is a Large Language Model?
#### There are 3 types of transformers :
1. エンコーダー
エンコーダーベースのトランスフォーマーは、テキスト (またはその他のデータ) を入力として受け取り、そのテキストの高密度表現 (または埋め込み) を出力します。

- 例: Google の BERT
- 使用例: テキスト分類、セマンティック検索、固有表現認識
- 標準サイズ: 数百万のパラメータ

2. デコーダー
デコーダーベースのトランスフォーマーは、一度に 1 つのトークンを生成してシーケンスを完了することに重点を置いています。

- 例: Meta の Llama
- 使用例: テキスト生成、チャットボット、コード生成
- 典型的なサイズ: 数十億（米国の意味では10^9）のパラメータ

3. Seq2Seq (エンコーダー - デコーダー)
シーケンスからシーケンスへのトランスフォーマーは、エンコーダーとデコーダーを組み合わせたものです。エンコーダーは最初に入力シーケンスをコンテキスト表現に処理し、次にデコーダーが出力シーケンスを生成します。

- 例: T5、BART、
- 使用例: 翻訳、要約、言い換え
- 標準サイズ: 数百万のパラメータ

> LLMの基本原理はシンプルだが、非常に効果的である。その目的は、前のトークンのシーケンスが与えられたときに、次のトークンを予測することである。 トークン」とは、LLMが扱う情報の単位である。 トークン」はあたかも「単語」のように考えることができますが、効率上の理由からLLMは単語全体を使用しません。

> 各LLMは、そのモデルに固有の特別なトークンをいくつか持っている。 LLMはこれらのトークンを使って、生成された構造化コンポーネントを開いたり閉じたりします。 例えば、シーケンス、メッセージ、レスポンスの開始や終了を示すためなどである。 さらに、モデルに渡す入力プロンプトも特別なトークンで構造化されている。 その中で最も重要なのがEOS（End of sequence token）である。

#### Understanding next token prediction.
> LLMは自己回帰的と言われ、あるパスの出力が次のパスの入力になる。 このループは、モデルが次のトークンをEOSトークンと予測するまで続き、その時点でモデルは停止することができる。

> - 入力テキストがトークン化されると、モデルは入力シーケンスにおける各トークンの位置と意味に関する情報を取り込んだシーケンスの表現を計算する。
> - この表現はモデルに取り込まれ、モデルは語彙内の各トークンがシーケンスの次のトークンである可能性をランク付けしたスコアを出力する。

#### 要約

このテキストは、大規模言語モデル（LLM）の仕組みとAIエージェントにおける役割を解説したものです。以下、より詳細に解説します。

**1. LLMの定義と種類:**

* **定義:** LLMは、膨大なテキストデータで学習された、人間の言語を理解・生成できるAIモデルです。  その能力は、文章の生成、翻訳、要約、質問応答など多岐に渡ります。
* **アーキテクチャ:**  ほとんどの現代のLLMはTransformerアーキテクチャに基づいています。これは、「Attention」メカニズムを用いて、入力テキストの各単語間の関係性を効率的に計算するものです。
* **種類:**
    * **エンコーダ型:** 入力テキストを意味表現（埋め込み）に変換します。BERTが代表例で、テキスト分類や意味検索などに利用されます。パラメータ数は数百万程度です。
    * **デコーダ型:** トークン列を生成します。Llamaが代表例で、テキスト生成、チャットボット、コード生成などに利用されます。パラメータ数は数十億規模です。
    * **Seq2Seq型:** エンコーダとデコーダの両方を組み合わせ、入力シーケンスを処理して出力シーケンスを生成します。翻訳や要約などに利用されます。パラメータ数は数百万程度です。
* **主要なLLM:** GPT-4 (OpenAI), Llama (Meta), Deepseek-R1 (DeepSeek), SmollLM2 (Hugging Face), Gemma (Google), Mistral (Mistral)などが例として挙げられています。


**2. LLMの動作原理:**

* **トークン化:**  LLMは、テキストを単語ではなく、より小さな単位である「トークン」に分割して処理します。これは、語彙数を削減し、効率を高めるためです。
* **次のトークン予測:** LLMの主要なタスクは、前のトークン列に基づいて、次に来るトークンを予測することです。これは、自己回帰的な（autoregressive）プロセスで、予測されたトークンが次の予測への入力となります。
* **Attentionメカニズム:**  Attentionメカニズムは、次のトークンを予測する際に、入力テキスト中のどの単語が最も重要かを判断するのに役立ちます。重要な単語に重みを付けて処理することで、予測精度を高めます。
* **デコーディング戦略:**  次のトークンを選択する方法は複数あります。最も単純な方法は、確率が最も高いトークンを選択することですが、Beam Searchなど、より高度な方法も存在します。
* **特殊トークン:**  シーケンスの開始・終了、メッセージの開始・終了などを示す特殊なトークンが使用されます。モデルによって特殊トークンは異なり、その多様性も示されています。


**3. LLMの学習:**

* **事前学習:** 大量のテキストデータを用いて、自己教師あり学習（masked language modeling）を行います。これは、テキストの一部をマスクして、そのマスクされた部分を予測させることで、言語構造やパターンを学習する手法です。
* **ファインチューニング:** 事前学習済みのモデルを、特定のタスク（対話、分類、コード生成など）に合わせたデータでさらに学習させることで、特定のタスクにおける性能を向上させます。


**4. LLMの利用方法:**

* **ローカル実行:** 十分な計算資源を持つ環境で、ローカルにLLMを実行できます。
* **クラウド/API:** Hugging Faceなどのプラットフォームが提供するAPIを利用することで、容易にLLMを利用できます。


**5. AIエージェントにおけるLLMの役割:**

LLMはAIエージェントにおいて、人間の言語を理解し生成する中核的な役割を果たします。ユーザーの指示を解釈し、コンテキストを維持し、計画を立て、必要なツールを選択するなど、エージェントの知的な行動を支えます。


**まとめ:**

このテキストは、LLMの基本的な仕組みから、AIエージェントにおける応用までを網羅的に解説しています。  LLMは、近年急速に発展している技術であり、その応用範囲はますます広がっています。  より深い理解のためには、文中にあるNLPコースの受講が推奨されています。

### Messages and Special Tokens
このテキストは、大規模言語モデル（LLM）がチャットインターフェースでどのように会話処理を行うかを説明しています。

要点は以下の通りです。

* **チャットにおけるメッセージの扱い**: ユーザーとLLM間のメッセージは、モデルへの入力前に単一のプロンプトに連結される。UI上はメッセージ単位に見えるが、内部的には一つのテキストシーケンスとして処理される。

* **チャットテンプレートの役割**: 異なるLLMは独自の特殊トークンを使用するため、チャットテンプレートは、ユーザーとアシスタントのメッセージをLLMが理解できる形式に整形する役割を果たす。システムメッセージ（モデルの振る舞い方を定義）もテンプレートで管理される。

* **ベースモデルとインストラクトモデル**: ベースモデルは生のテキストデータで学習され、インストラクトモデルは指示に従うよう微調整されている。ベースモデルをインストラクトモデルのように動作させるには、適切なプロンプトフォーマット（チャットテンプレート）が必要。

* **`transformers`ライブラリ**:  `transformers`ライブラリは、チャットテンプレートを自動的に処理し、メッセージリストをモデルに入力可能なプロンプトに変換する機能を提供する(`apply_chat_template`関数)。

要するに、LLMとの自然な会話を実現するために、バックエンドではメッセージの連結と適切なフォーマットが重要であり、そのための仕組みとしてチャットテンプレートが使用されているということです。

#### Messages: The Underlying System of LLMs
##### System Messages
##### Conversations: User and Assistant Messages
#### Chat-Templates
##### Base Models vs. Instruct Models
##### Understanding Chat Templates
##### Messages to prompt

### What are Tools?
#### 要約
この文章は、AIエージェントが外部ツールを利用して機能を拡張する方法を説明しています。大きく分けて以下の３つのパートに構造化できます。

**パート１：ツールの定義と種類**

* **ツールの定義:**  AIエージェントは、大規模言語モデル (LLM) に「ツール」を提供することで、その能力を拡張します。ツールとは、LLMがテキスト入力を受け取り、テキスト出力を生成するという制約を超えて、外部データの取得や計算などのアクションを実行するための関数です。

* **ツールの種類 (例):**
    * **Web 検索:** インターネットから最新の情報を取得します。
    * **画像生成:** テキストの説明に基づいて画像を作成します。
    * **情報検索:** 外部ソースから情報を取得します。
    * **API インターフェース:** GitHub、YouTube、Spotifyなどの外部APIと対話します。

* **良いツールの条件:** LLMの能力を補完するものであり、LLM単体では扱えない最新データや計算能力などを提供する必要があります。例として、算術計算には電卓ツールが挙げられています。

**パート２：ツールの動作と実装**

* **ツールの動作:** LLMはツールを直接呼び出すことができません。エージェントが、LLMからのテキスト指示を解析し、適切なツールを呼び出して実行し、結果をLLMに戻します。この過程はユーザーには隠蔽されます。

* **ツールへの情報の提供:** LLMにツールを認識させるには、システムプロンプトにツールの詳細な情報を記述する必要があります。この情報には、ツールの名前、説明、入力、出力の型などが含まれます。

* **ツール記述の自動生成:** Pythonコードを用いたツールの自動生成方法が提示されています。型ヒント、docstring、関数名を利用して、ツール情報を自動的に生成するデコレータ`@tool`が紹介されています。

**パート３：まとめと今後の展望**

* **ツールの重要性:**  LLMの静的なトレーニングデータの限界を超え、リアルタイムのタスクや特殊なアクションを可能にします。

* **今後の展望:** エージェントのワークフロー（観測、思考、行動）について、より詳細な説明が続くことが示唆されています。

#### What are AI Tools?
AI エージェントでよく使用されるツール

| ツール            | 説明                                                |
| ----------------- | --------------------------------------------------- |
| ウェブ検索        | エージェントがインターネットから最新情報を取得できるようにします。 |
| 画像生成          | テキストの説明に基づいて画像を作成します。                  |
| 検索              | 外部ソースから情報を取得します。                            |
| APIインターフェース | 外部 API (GitHub、YouTube、Spotify など) とやり取りします。     |

#### How do tools work?
#### How do we give tools to an LLM?

##### Auto-formatting Tool sections
##### Generic Tool implementation

### Understanding AI Agents through the Thought-Action-Observation Cycle
この文章は、AIエージェントの動作原理を「思考-行動-観察（Thought-Action-Observation）」サイクルに基づいて解説しています。  以下の構造で説明できます。

**1. はじめに:**

* 前提として、システムプロンプトによるツールの提供方法と、AIエージェントの推論・計画・環境との相互作用能力について触れられています。

**2. 思考-行動-観察サイクルの中核:**

* AIエージェントは、思考→行動→観察という連続したループで動作します。これは、目標達成まで続くwhileループに相当します。
* 各ステップの詳細：
    * **思考（Thought）:** LLM（大規模言語モデル）が次のステップを決定します。問題を分解し、必要なツールやパラメータを検討します。
    * **行動（Action）:** ツールを呼び出し、必要な引数を渡します。
    * **観察（Observation）:** ツールからの応答を反映し、結果を評価します。これは環境からのフィードバックです。

**3. システムプロンプトとサイクルの組み込み:**

* システムプロンプトには、エージェントの行動規範、利用可能なツール、思考-行動-観察サイクル自体が組み込まれています。

**4. 例：天気予報エージェントAlfred:**

* ユーザーの質問「ニューヨークの今日の天気は？」に対するAlfredの動作例が示されています。
* Alfredは、思考段階でAPI呼び出しを決定し、行動段階で天気APIツールを呼び出します。
* 観察段階でAPIからのレスポンスを受け取り、その結果に基づいて最終的な回答を生成します。
* この例では、エラー発生時やデータ不備時のサイクル再実行の可能性も示唆されています。

**5.  サイクルの特徴と利点:**

* **反復処理:** 目標達成までループを繰り返します。
* **ツール統合:** 外部ツール（天気APIなど）との連携により、リアルタイムデータの取得が可能です。
* **動的適応:** 観察結果に基づいて思考を修正し、正確性を高めます。

**6. まとめ:**

* 思考-行動-観察サイクルが、複雑なタスクを段階的に解決するAIエージェントの能力を支えていることが強調されています。  このサイクルは、次のセクションでより詳細に説明される「ReActサイクル」の基礎となっています。


全体として、この文章は、抽象的なAIエージェントの概念を、具体的な例を用いて分かりやすく説明することを目的としています。  特に、システムプロンプトと外部ツールとの連携が、AIエージェントの機能を拡張する上で重要であることを示しています。


#### The Core Components
エージェントは、思考（Thought）→ 行動（Act）および観察（Observe）という継続的なサイクルで動作します。

これらのアクションを一緒に分解してみましょう。

1. 思考: エージェントの LLM 部分が次のステップが何であるかを決定します。
2. 行動: エージェントは、関連付けられた引数を使用してツールを呼び出すことによってアクションを実行します。
3. 観察: モデルはツールからの応答を反映します。

#### The Thought-Action-Observation Cycle

#### Alfred, the weather Agent
##### Thought
##### Action
##### Observation
##### Updated thought
##### Final Action

### Thought, Internal Reasoning and the Re-Act Approach
#### 要約
この文章は、AIエージェントの内部動作、特に推論と計画能力について説明しています。  重要なのは、「Re-Actアプローチ」という手法で、これは「考える（Think）」「行動する（Act）」を組み合わせたものです。「Let's think step by step」というプロンプトによって、モデルが段階的に問題を解決するよう促します。これにより、複雑な問題を小さなステップに分解し、エラーを減らすことができます。  また、エージェントは「思考（Thought）」を通して、計画、分析、意思決定、問題解決、記憶統合、自己省察、目標設定、優先順位付けなどを行い、内部的な対話を通してタスクを遂行します。  関数呼び出しを調整されたLLMでは、この思考プロセスは省略可能な場合もあると記述されています。


| 思考の種類 | 例 |
| ---------- | ------------------------------------------------------------ |
| 計画       | 「このタスクを 3 つのステップに分割する必要があります: 1) データの収集、2) 傾向の分析、3) レポートの生成」 |
| 分析       | 「エラーメッセージによると、問題はデータベース接続パラメータにあるようです」 |
| 意思決定   | 「ユーザーの予算の制約を考慮すると、中間層のオプションを推奨します」 |
| 問題解決   | 「このコードを最適化するには、まずプロファイリングしてボトルネックを特定する必要があります」 |
| メモリ統合 | 「先ほどユーザーが Python を好むとおっしゃっていたので、Python の例を示します」 |
| 自己反省   | 「前回のアプローチはうまくいかなかったので、別の戦略を試してみる必要がある」 |
| 目標設定   | 「このタスクを完了するには、まず受け入れ基準を確立する必要があります」 |
| 優先順位   | 「新しい機能を追加する前にセキュリティの脆弱性に対処する必要がある」 |

#### The Re-Act Approach

### Actions, Enabling the Agent to Engage with Its Environment
#### 要約
このテキストは、AIエージェントが環境とどのように相互作用するか、特に「行動（Actions）」に焦点を当てて説明しています。

主なポイントは次の通りです。

* **エージェントの種類**: JSON形式、コード、関数呼び出しの3種類のエージェントがあり、それぞれ異なる方法で行動を表現します。JSONエージェントはJSONで行動を記述し、コードエージェントは実行可能なコードを生成します。関数呼び出しエージェントは、各アクションのために新しいメッセージを生成するように微調整されたJSONエージェントの一種です。

* **行動の種類**: 情報収集、ツール使用、環境操作、コミュニケーションなど、様々なタイプの行動があります。

* **停止と解析のアプローチ**: エージェントは、行動が完了したら生成を停止し（stop）、外部のパーサーがその出力（JSONやコード）を解析（parse）します。これにより、エージェントの応答が明確で正確になります。  これは、すべてのエージェントの種類で共通の重要な手順です。

* **コードエージェントの利点**:  コードエージェントは、複雑なロジックを表現でき、モジュール性が高く、デバッグが容易という利点があります。

要約すると、このテキストは、AIエージェントが環境と効果的にやり取りするための、行動の表現方法と処理方法について解説しています。  特に「停止と解析」のアプローチが、正確で効率的なエージェントの動作に重要であることを強調しています。

> アクションとは、AI エージェントが環境と対話するために実行する具体的なステップです。

情報を求めて Web を閲覧する場合でも、物理デバイスを制御する場合でも、各アクションはエージェントによって実行される意図的な操作です。

#### Types of Agent Actions

| エージェントの種類 | 説明 |
|---|---|
| JSONエージェント | 実行するアクションはJSON形式で指定されます |
| コードエージェント | エージェントは外部で解釈されるコードブロックを記述します |
| 関数呼び出しエージェント | これは、各アクションごとに新しいメッセージを生成するように微調整されたJSONエージェントのサブカテゴリです。 |

| アクションの種類 | 説明 |
|---|---|
| 情報収集 | Web 検索、データベースのクエリ、またはドキュメントの取得を実行します。 |
| ツールの使用 | API 呼び出し、計算の実行、コードの実行。 |
| 環境との相互作用 | デジタル インターフェースを操作したり、物理デバイスを制御したりします。 |
| コミュニケーション | チャットを通じてユーザーと交流したり、他のエージェントと共同作業したりします。 |

#### The Stop and Parse Approach
#### Code Agents
Agent の一種ってことだな。JSON などを生成する代わりにコードを生成する  
メリットは以下の通り

- 表現力:コードはループ、条件文、ネストされた関数などの複雑なロジックを自然に表現できるため、JSON よりも柔軟性が高くなります。
- モジュール性と再利用性:生成されたコードには、さまざまなアクションやタスクで再利用できる関数やモジュールを含めることができます。
- デバッグ性の向上:プログラミング構文が明確に定義されているため、コード エラーの検出と修正が容易になります。
- 直接統合:コード エージェントは外部ライブラリや API と直接統合できるため、データ処理やリアルタイムの意思決定などのより複雑な操作が可能になります。

### Observe, Integrating Feedback to Reflect and Adapt
（短いので要約は作成しない）

> 観察とは、エージェントが自身の行動の結果をどのように認識するかということです。
>
> これらは、エージェントの思考プロセスを促進し、将来の行動を導く重要な情報を提供します。

---

> 観察フェーズでは、エージェントは次の作業を行います。
> 
> - フィードバックを収集します:アクションが成功したか失敗したかを示すデータまたは確認を受け取ります。
> - 結果の追加:新しい情報を既存のコンテキストに統合し、メモリを効果的に更新します。
> - 戦略を適応させる:更新されたコンテキストを使用して、その後の考えや行動を洗練させます。

| 観察の種類 | 例 |
|---|---|
| システムフィードバック | エラーメッセージ、成功通知、ステータスコード |
| データの変更 | データベースの更新、ファイルシステムの変更、状態の変更 |
| 環境データ | センサーの読み取り値、システムメトリック、リソースの使用状況 |
| レスポンス分析 | APIレスポンス、クエリ結果、計算出力 |
| 時間ベースのイベント | 期限が到来し、予定されたタスクが完了しました |

#### How Are the Results Appended?
アクションを実行した後、フレームワークは次の手順を順番に実行します。

1. アクションを解析して、呼び出す関数と使用する引数を識別します。
2. アクションを実行します。
3. 結果をObservationとして追加します。

### Dummy Agent Library
#### 要約
このコースは、AIエージェントの概念に焦点を当て、特定のフレームワークにこだわらないよう、フレームワーク非依存です。Unit 1では、ダミーのエージェントライブラリとシンプルなサーバーレスAPIを使用してLLMエンジンにアクセスします。本番環境では使用しませんが、エージェントの動作を理解するための良い出発点となります。その後、smolagents、LangGraph、LangChain、LlamaIndexなどのAIエージェントライブラリを使用します。簡単なPython関数と組み込みパッケージ(datetime、os)でツールとエージェントを作成し、Hugging FaceのServerless APIを用いてLlama-3.2-3B-Instructモデルと対話します。

チャットモデルでは、適切なプロンプトテンプレート（特殊トークン）を使用しないと、モデルが予期せぬ出力を生成することが示されました。「chat」メソッドが推奨されていますが、学習目的のため「text_generation」メソッドを使用し、プロンプトを手動で調整する方法を学びます。

ダミーのエージェントでは、システムプロンプトに情報を追加することで動作します。天気情報を取得するダミー関数`get_weather`を作成し、LLMの出力を関数実行結果で更新することで、現実的なエージェント動作を実現する方法を説明しています。  LLM単体では事実を「幻覚」する可能性があり、ツールと連携させることで正確な情報を取得できることを示しています。最終的に、より高度なエージェントライブラリ（smolagentsなど）に移行する準備が整います。


#### Dummy Agent
このシステム・プロンプトは、先に見たものよりも少し複雑だが、すでに以下の内容が含まれている。

1. ツールに関する情報
2. サイクルの指示（思考→行動→観察）

ここまで1時間半くらい。なんだかんだで4時間くらいかかったかな

### Let’s Create Our First Agent Using Smolagents
#### 要約

この内容は、smolagentsライブラリを使ってHugging Face Space上で動作するシンプルなコードエージェントを作成するチュートリアルです。

**要点:**

* **smolagents:** エージェント開発を簡素化する軽量ライブラリ。コードブロックの実行による「行動」と結果の「観察」を繰り返す`CodeAgent`に焦点を当てている。
* **Hugging Face Spaceテンプレート:** 提供されたテンプレートを複製して、自分のSpaceでエージェントを作成する。
* **`app.py`:** エージェントの動作を定義するメインのPythonファイル。ここを編集してツールを追加していく。
* **ツール:** エージェントが持つ機能。`@tool`デコレータを使って定義する。テンプレートにはダミーのツールとタイムゾーン取得ツールが例として含まれている。`DuckDuckGoSearchTool`や`image_generation_tool`のような既存ツールも利用可能。
* **LLM:** `Qwen/Qwen2.5-Coder-32B-Instruct`が使用されている。
* **目標:** 提供されたテンプレートにツールを追加し、エージェントを実際に動作させてみる。既存のツールを使うだけでなく、独自のツールを作成してみるのも良い。完成したエージェントはdiscordの#agents-course-showcaseで共有することが推奨されている。

チュートリアルでは、まずテンプレートを複製し、`app.py`内の`tools`パラメータにツールを追加することでエージェントに機能を追加していく流れとなっています。  最終的には、自分で作成したエージェントを公開・共有することが目標です。

#### What is smolagents?
https://youtu.be/PQDKcWiuln4?si=mWHHHX8g1TAmReUw

#### Let’s build our Agent!
https://huggingface.co/spaces/agents-course/First_agent_template これを複製する

https://huggingface.co/spaces/noriyotcp/First_agent_template

`app.py` を編集していく

`CodeAgent` class from `smolagents` を使用する

##### The Tools
2つ提供されている

1. ダミーツール
2. 実際に動くツール。世界のどこかの現在時刻を取得する

##### The Agent
[Qwen/Qwen2.5-Coder-32B-Instruct](https://huggingface.co/Qwen/Qwen2.5-Coder-32B-Instruct) を使用する

`CodeAgent` のパラメータである `tools` に新しいツールを追加していく

Space と Agent に詳しくなることがゴールだよ。現在エージェントはなんのツールも使ってないので出来上がってるやつとか新しいツールとか追加していくよ

### Unit 1 Final Quiz
80% 正解で certification がもらえる。最後 Submit ボタンを押さないといけないが本当に送信されてるかどうかわかりにくい

### Conclusion
次のユニットは 2/18 だ！ その前に Bonus Unit: Fine-tune your agent がある。これを忘れていた。

前回から大体ここまで50分くらい。合計5時間くらい
