---
title: "Hugging Face Agents Course Bonus Unit 2 - AI Agent Observability & Evaluation"
date: "2025-06-19 20:08:34 +0900"
last_modified_at: "2025-06-19 20:08:34 +0900"
---

# Hugging Face Agents Course Bonus Unit 2 - AI Agent Observability & Evaluation
## Introduction
1.  このコースは、AIエージェントに関するコースであり、エージェントの可観測性と評価に焦点を当てている。コースは、AIエージェントの基本的な概念の紹介から始まり、様々なフレームワーク（smolagents、LlamaIndex、LangGraphなど）の検討を経て、具体的なエージェントの応用事例（Agentic RAG）へと進む。
2.  コースの中核は、受講者が実際にAIエージェントを作成、テスト、そして認定を行う最終プロジェクトである。これにより、理論だけでなく実践的なスキルも習得できる。
3.  コースにはボーナスユニットも用意されており、ここではLLM（大規模言語モデル）の関数呼び出しのためのファインチューニング、エージェントの可観測性と評価、そしてゲーム（ポケモン）におけるエージェントの応用といった、より高度なトピックが扱われる。

### 📚 When Should I Do This Bonus Unit?
### 🤓 What You’ll Learn
- エージェントを計装する： OpenTelemetry経由で観測可能なツールをsmolagentsフレームワークと統合する方法を学びます。
- メトリクスを監視する： トークン使用量（コスト）、待ち時間、エラートレースなどのパフォーマンス指標を追跡します。
- リアルタイムで評価する: ユーザのフィードバックを収集したり、LLM-as-a-judgeを活用するなど、ライブ評価のテクニックを理解します。
- オフライン分析： ベンチマークデータセット（例：GSM8K）を使用して、エージェントのパフォーマンスをテスト・比較します。

### 🚀 Ready to Get Started?

## AI Agent Observability and Evaluation
1.  **AIエージェントの可観測性（Observability）とは、ログ、メトリクス、トレースなどの外部シグナルを監視して、AIエージェント内部で何が起こっているかを理解することです。** これは、エージェントのパフォーマンスをデバッグおよび改善するために、アクション、ツール利用、モデル呼び出し、応答を追跡することを意味します。可観測性がない場合、AIエージェントはブラックボックスとなり、コスト、精度、レイテンシ、有害な言語の検出、プロンプトインジェクションの監視、ユーザーフィードバックの追跡が困難になります。

2.  **AIエージェントの可観測性を実現するためのツールとして、LangfuseやArizeなどのプラットフォームがあります。** これらのツールは、詳細なトレースを収集し、メトリクスをリアルタイムで監視するためのダッシュボードを提供します。これにより、問題の検出とパフォーマンスの最適化が容易になります。これらのツールは機能や能力が異なり、オープンソースのものもあれば、特定のLLMOps（LLMの運用）に特化したものもあります。多くのエージェントフレームワーク（例：smolagents）は、OpenTelemetry標準を使用してメタデータを可観測性ツールに公開します。

3.  **可観測性ツールは、エージェントの実行をトレースとスパンで表現します。** トレースは、ユーザーからのクエリ処理など、エージェントのタスクの開始から終了までを表します。スパンは、言語モデルの呼び出しやデータ検索など、トレース内の個々のステップを表します。

4.  **AIエージェントを監視するための重要なメトリクスには、レイテンシ、コスト、リクエストエラー、ユーザーフィードバック、精度などがあります。** レイテンシはエージェントの応答速度、コストはエージェントの実行ごとの費用、リクエストエラーはAPIエラーやツール呼び出しの失敗回数、ユーザーフィードバックは明示的（評価、コメント）および暗示的（質問の言い換え、再クエリ）なユーザーの行動、精度はエージェントが出力する結果の正しさや望ましさの度合いを示します。これらのメトリクスを監視することで、エージェントの健全性を総合的に把握できます。

5.  **AIエージェントの評価は、可観測性によって得られたデータ（およびテスト）を分析して、エージェントのパフォーマンスを判断し、改善方法を決定するプロセスです。** AIエージェントは非決定的であり、更新やモデルのドリフトによって変化する可能性があるため、定期的な評価が重要です。評価には、オフライン評価とオンライン評価の2種類があり、相互に補完し合います。



8.  **AIエージェントの評価を成功させるためには、オンラインとオフラインの両方の方法を組み合わせることが重要です。** 定期的なオフラインベンチマークを実施して、定義されたタスクに対するエージェントのスコアを定量的に評価し、ライブの利用状況を継続的に監視して、ベンチマークでは見逃される事柄を捕捉します。多くのチームが、オフライン評価 → 新しいエージェントバージョンのデプロイ → オンラインメトリクスの監視と新しい失敗例の収集 → オフラインテストセットへの例の追加 → 反復というループを採用しています。

### 🔎 What is Observability?
### 🔭 Why Agent Observability Matters
### 🔨 Observability Tools
smolagentsのような多くのエージェントフレームワークは、OpenTelemetry標準を使用して、観測可能性ツールにメタデータを公開しています。これに加えて、観測可能なツールは、LLMの動きの速い世界でより柔軟性を持たせるために、カスタムインスツルメンテーションを構築します。何がサポートされているかは、使用しているツールのドキュメントを確認してください。

Quiz より：  

> OpenTelemetry は、エージェントの動作を監視および診断するために不可欠なテレメトリ データの計測を標準化します。

### 🔬Traces and Spans
### 📊 Key Metrics to Monitor
### 👍 Evaluating AI Agents
AIエージェントの評価には、オンライン評価とオフライン評価の2つのカテゴリーがある。どちらも価値があり、互いに補完し合うものです。これは、エージェントを配備する前に最低限必要なステップだからです。


#### 🥷 Offline Evaluation
6.  **オフライン評価は、テストデータセットを使用して、制御された環境でエージェントを評価することです。** 期待される出力や正しい動作が既知のデータセットを使用し、エージェントをそれらに対して実行します。これにより、再現性があり、明確な精度メトリクスを得ることができます。課題は、テストデータセットが包括的で、現実世界のシナリオを反映したものであることを保証することです。

#### 🔄 Online Evaluation
7.  **オンライン評価は、実際の利用環境でエージェントを評価することです。** ライブのユーザーインタラクションに対するエージェントのパフォーマンスを監視し、結果を継続的に分析します。これにより、ラボ環境では予測できない事柄を把握し、モデルのドリフトや予期しないクエリを検出できます。課題は、ライブインタラクションに対する信頼性の高いラベルやスコアを得ることが難しいことです。

これは、ライブで実世界の環境、すなわち本番での実際の使用中にエージェントを評価することを指します。オンライン評価では、実際のユーザインタラクションにおけるエージェントのパフォーマンスを監視し、結果を継続的に分析します。

例えば、成功率、ユーザ満足度、その他のメトリクスをライブトラフィックで追跡することができます。オンライン評価の利点は、ラボ環境では予期しないことを捉えることができることです。（入力パターンが変化するにつれてエージェントの有効性が低下する場合）時間の経過とともにモデルのドリフトを観察したり、テストデータにはなかった予期しないクエリや状況を捉えることができます。また、テストデータにはない予期せぬクエリや状況を把握することができます。

オンライン評価では、多くの場合、暗黙的・明示的なユーザフィードバックを収集し、シャドウテストやA/Bテスト（エージェントの新バージョンを並行して実行し、旧バージョンと比較する）を実施します。課題は、ライブのインタラクションに対して信頼できるラベルやスコアを得ることが難しいことです - ユーザからのフィードバックや下流のメトリクス（ユーザが結果をクリックしたかなど）に頼るかもしれません。

#### 🤝 Combining the two
実際には、AIエージェントの評価はオンラインとオフラインの手法をうまく組み合わせて行います。定義されたタスクについてエージェントを定量的にスコアリングするために定期的にオフラインベンチマークを実行し、ベンチマークが見逃すものをキャッチするためにライブの使用状況を継続的に監視することができます。例えば、オフラインテストは、既知の一連の問題に対するコード生成エージェントの成功率が向上しているかどうかをキャッチすることができます。一方、オンラインモニタリングは、エージェントが苦手とする新しいカテゴリーの質問をユーザがし始めたことを警告するかもしれません。両方を組み合わせることで、より強固な全体像を把握することができます。

実際、多くのチームが次のようなループを採用しています：オフライン評価→新しいエージェントバージョンのデプロイ→オンラインメトリクスの監視と新しい失敗例の収集→オフラインテストセットにそれらの例を追加→繰り返し。このように、評価は継続的であり、常に改善され続けます。

### 🧑💻 Lets see how this works in practice

## Bonus Unit 2: Observability and Evaluation of Agents
https://huggingface.co/learn/agents-course/bonus-unit2/monitoring-and-evaluating-agents-notebook

これは実際に手を動かして学んでいくようだな

### Exercise Prerequisites 🏗️
### Step 0: Install the Required Libraries
opentelemetry とか langfuse とか入れてる

:memo: ここら辺に出てくるライブラリを調べる
Langfuse のアカウント登録とか必要なのか。一旦読むだけにしよ

1.  このテキストでは、AIエージェントの内部ステップ（トレース）を監視し、オープンソースの可観測性ツールを使用してそのパフォーマンスを評価する方法について説明しています。エージェントの動作を観察および評価する能力は、タスクの失敗や最適でない結果のデバッグ、リアルタイムでのコストとパフォーマンスの監視、継続的なフィードバックを通じた信頼性と安全性の向上に不可欠です。

2.  可観測性ツールとしてLangfuseを使用し、OpenTelemetry互換の他のサービスも利用可能であることを説明しています。Langfuseを設定するために必要な環境変数の設定方法と、smolagentを計測する方法を示しています。また、LlamaIndexやLangGraphを計測するためのドキュメントへのリンクも提供しています。Hugging Faceトークンも設定する必要があることを示しています。

3.  OpenTelemetryを用いてトレースプロバイダーを設定し、smolagentsのInstrumentationを行う方法を解説しています。これにより、エージェントの実行時に生成されるログやスパンを可観測性ダッシュボードで確認できるようになります。簡単なCodeAgentの例を用いて、計測が正しく機能していることを確認する方法を示しています。

4.  より複雑なエージェント（DuckDuckGoSearchToolを使用するCodeAgent）を実行し、トークン使用量、レイテンシ、コストなどの高度なメトリクスがどのように追跡されるかを示しています。トレース構造について説明し、エージェントの実行、ツール呼び出し、LLM呼び出しなどのスパンが含まれていることを解説しています。

5.  オンライン評価の重要性を強調し、本番環境でのエージェントの監視と評価の方法を説明しています。コスト、レイテンシ、ユーザーフィードバック（サムズアップ/ダウン）、LLM-as-a-Judgeなどの一般的なメトリクスについて解説しています。ユーザーインターフェースにエージェントを組み込む方法の例として、Gradioを使用してチャットインターフェースを作成し、ユーザーフィードバックを記録する方法を示しています。

6.  LLM-as-a-Judgeを利用して、エージェントの出力を自動的に評価する方法について説明しています。評価テンプレートを定義し、エージェントの出力が特定の基準（毒性、正確性、スタイルなど）を満たしているかを判断するために別のLLMを使用する方法を示しています。

7.  オフライン評価の重要性を強調し、開発前または開発中に系統的なチェックを行うことで、品質と信頼性を維持する方法を説明しています。ベンチマークデータセット（プロンプトと期待される出力のペア）を使用してエージェントを実行し、出力を比較する方法を示しています。GSM8Kデータセットを使用して、数学の問題とその解答に対するエージェントのパフォーマンスを評価する例を示しています。Langfuseにデータセットを登録し、各項目をシステムに追加する方法を解説しています。

8.  データセット上でエージェントを実行するヘルパー関数`run_smolagent()`を定義し、OpenTelemetryスパンを開始し、エージェントを実行し、トレースIDをLangfuseに記録する方法を説明しています。データセットの各項目に対してエージェントを実行し、トレースをデータセット項目にリンクする方法を示しています。

9.  異なるモデル、ツール、プロンプトを使用してこのプロセスを繰り返し、可観測性ツールでそれらを比較する方法を解説しています。最後に、smolagentsとOpenTelemetryエクスポーターを使用した可観測性の設定、計測の確認、詳細なメトリクスのキャプチャ、ユーザーフィードバックの収集、LLM-as-a-Judgeの使用、ベンチマークデータセットを使用したオフライン評価の方法についてまとめています。

### Step 1: Instrument Your Agent
### Step 2: Test Your Instrumentation
### Step 3: Observe and Evaluate a More Complex Agent
#### Trace Structure
### Online Evaluation
#### Common Metrics to Track in Production
##### 1. Costs
##### 2. Latency
##### 3. Additional Attributes
##### 4. User Feedback
##### 5. LLM-as-a-Judge
##### 6. Observability Metrics Overview
### Offline Evaluation
#### Dataset Evaluation
##### Running the Agent on the Dataset
### Final Thoughts


